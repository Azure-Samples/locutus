{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sources.json\", \"r\") as f:\n",
    "    sources = json.load(f)[\"sources\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(s, ignore):\n",
    "    r = []\n",
    "    for a in ignore.keys():\n",
    "        if not hasattr(str, a):\n",
    "            continue\n",
    "\n",
    "        fn = getattr(str, a)\n",
    "        if type(ignore[a]) == str:\n",
    "            r += [fn(s, ignore[a])]\n",
    "\n",
    "        elif type(ignore[a]) == list:\n",
    "            r += [fn(s, i) for i in ignore[a]]\n",
    "\n",
    "        elif type(ignore[a]) == bool and ignore[a]:\n",
    "            r += [fn(s)]\n",
    "\n",
    "    return any(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitue(s, replace):\n",
    "    for a in replace.keys():\n",
    "        s = re.sub(a, replace[a], s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(\n",
    "    title=\"\",\n",
    "    source=\"\",\n",
    "    start=0,\n",
    "    end=100,\n",
    "    ignore={},\n",
    "    replace={},\n",
    "    cache=\".cache\",\n",
    "    force=False,\n",
    "):\n",
    "    print(f\"Loading {title}\")\n",
    "    # get filename\n",
    "    a = urlparse(source)\n",
    "    file = os.path.basename(a.path)\n",
    "\n",
    "    # check cache\n",
    "    c = Path(cache).absolute().resolve()\n",
    "    if not c.exists():\n",
    "        os.makedirs(str(c))\n",
    "    cfile = c.joinpath(file)\n",
    "    if force or not cfile.exists():\n",
    "        response = requests.get(source)\n",
    "        with open(str(cfile), \"wt\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # load text\n",
    "    with open(str(cfile), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    lines = text.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\").split(\"\\n\")[start:end]\n",
    "\n",
    "    # cleaned sentences\n",
    "    sentences = [\n",
    "        f\"{s.strip()}.\"\n",
    "        for s in \" \".join(\n",
    "            [\n",
    "                substitue(item, replace).strip()\n",
    "                for item in lines\n",
    "                if len(item) > 0 and not check(item, ignore)\n",
    "            ]\n",
    "        ).split(\".\")\n",
    "    ]\n",
    "    print(\"Done!\")\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/homer.raw.txt\", \"w\") as f:\n",
    "    for source_id in [0, 1]:\n",
    "        text =  load(**sources[source_id])\n",
    "        for line in text:\n",
    "            print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning model \n",
    "\n",
    "Based on https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"./model\").resolve()\n",
    "config_path = root_dir / \"config\"\n",
    "model_path = root_dir / \"weights\"\n",
    "tokenizer_path = root_dir / \"tokenizer\"\n",
    "cache_dir = root_dir / \".cache\"\n",
    "output_dir = root_dir / \".outputs\"\n",
    "data_path = Path(\"./data\").resolve() / \"homer.raw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and process it\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {}\n",
    "dataset_args = {}\n",
    "data_files[\"train\"] = str(data_path)\n",
    "extension = \"text\"\n",
    "dataset_args[\"keep_linebreaks\"] = True\n",
    "raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=cache_dir, **dataset_args)\n",
    "\n",
    "# train:val split = 80:20\n",
    "validation_split_percentage = 20\n",
    "raw_datasets[\"validation\"] = load_dataset(\n",
    "    extension,\n",
    "    data_files=data_files,\n",
    "    split=f\"train[:{validation_split_percentage}%]\",\n",
    "    cache_dir=cache_dir,\n",
    "    **dataset_args,\n",
    ")\n",
    "raw_datasets[\"train\"] = load_dataset(\n",
    "    extension,\n",
    "    data_files=data_files,\n",
    "    split=f\"train[{validation_split_percentage}%:]\",\n",
    "    cache_dir=cache_dir,\n",
    "    **dataset_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast_tokenizer=True, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config_path, cache_dir=cache_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "# generate text from prefix before fine-tuning\n",
    "device = -1 if model.device.type == \"cpu\" else model.device.index\n",
    "text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "print(text_generator(\"The war in\")[0][\"generated_text\"])\n",
    "print(text_generator(\"The market in America\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# initialize traing arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(output_dir), \n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    eval_accumulation_steps = 1,\n",
    "    num_train_epochs = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, default_data_collator, is_torch_tpu_available\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    "    if training_args.do_eval and not is_torch_tpu_available()\n",
    "    else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    max_train_samples = len(train_dataset)\n",
    "   \n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# eval\n",
    "if training_args.do_eval:\n",
    "    metrics = trainer.evaluate()\n",
    "    max_eval_samples = len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "    metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "# generate text from prefix after fine-tuning\n",
    "device = -1 if model.device.type == \"cpu\" else model.device.index\n",
    "text_generator = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "print(text_generator(\"The war in\")[0][\"generated_text\"])\n",
    "print(text_generator(\"The market in America\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbaf699705a9d2c0424d3e39b0fb26cef1fc49f20dae9718b3681441105d6206"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
